{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "334417ae",
   "metadata": {},
   "source": [
    "## Check out KerasNLP Docs!!!\n",
    "https://keras.io/examples/nlp/parameter_efficient_finetuning_of_gpt2_with_lora/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190ab9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import keras_nlp\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "policy = keras.mixed_precision.Policy(\"mixed_float16\")\n",
    "keras.mixed_precision.set_global_policy(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e3d31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_train, imdb_test = tfds.load(\n",
    "    \"imdb_reviews\",\n",
    "    split=[\"train\", \"test\"],\n",
    "    as_supervised=True,\n",
    "    batch_size=16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f625cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a BERT model.\n",
    "lora_model = keras_nlp.models.BertClassifier.from_preset(\n",
    "    \"bert_small_en_uncased\", num_classes=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5744d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the original model.\n",
    "preprocessor = keras_nlp.models.BertPreprocessor.from_preset(\n",
    "    \"bert_small_en_uncased\",\n",
    "    sequence_length=128,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3600f66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(sentences):\n",
    "    softmax = tf.keras.activations.softmax\n",
    "    preds = softmax(lora_model(preprocessor(sentences)))\n",
    "    for sentence, pred in zip(sentences,preds):\n",
    "        neg, pos = pred.numpy()\n",
    "        print(sentence)\n",
    "        print(f\"Negative:\\t{neg}\\nPositive:\\t{pos}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4e92e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_sentiment(\n",
    "    [\"What an amazing movie!\", \"A total waste of my time.\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49517df3",
   "metadata": {},
   "source": [
    "### What exactly is LoRA?\n",
    "\n",
    "LoRA is a parameter-efficient fine-tuning technique for LLMs. It freezes the\n",
    "weights of the LLM, and injects trainable rank-decomposition matrices. Let's\n",
    "understand this more clearly.\n",
    "\n",
    "Assume we have an `n x n` pre-trained dense layer (or weight matrix), `W0`. We\n",
    "initialize two dense layers, `A` and `B`, of shapes `n x rank`, and `rank x n`,\n",
    "respectively. `rank` is much smaller than `n`. In the paper, values between 1\n",
    "and 4 are shown to work well.\n",
    "\n",
    "\n",
    "#### LoRA equation\n",
    "\n",
    "The original equation is `output = W0x + b0`, where `x` is the input, `W0` and\n",
    "`b0` are the weight matrix and bias terms of the original dense layer (frozen).\n",
    "The LoRA equation is: `output = W0x + b0 + BAx`, where `A` and `B` are the\n",
    "rank-decomposition matrices.\n",
    "\n",
    "LoRA is based on the idea that updates to the weights of the pre-trained\n",
    "language model have a low \"intrinsic rank\" since pre-trained language models are\n",
    "over-parametrized. Predictive performance of full fine-tuning can be replicated\n",
    "even by constraining `W0`'s updates to low-rank decomposition matrices.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://i.imgur.com/f4TFqMi.png\" alt=\"lora_diagram\" height=\"250\"/>\n",
    "</p>\n",
    "<br>\n",
    "\n",
    "#### Number of trainable parameters\n",
    "\n",
    "Let's do some quick math. Suppose `n` is 768, and `rank` is 4. `W0` has\n",
    "`768 x 768 = 589,824` parameters, whereas the LoRA layers, `A` and `B` together\n",
    "have `768 x 4 + 4 x 768 = 6,144` parameters. So, for the dense layer, we go from\n",
    "`589,824` trainable parameters to `6,144` trainable parameters!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837db264",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoraLayer(tf.keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        original_layer,\n",
    "        rank=8,\n",
    "        alpha=32,\n",
    "        trainable=False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        # We want to keep the name of this layer the same as the original\n",
    "        # dense layer.\n",
    "        original_layer_config = original_layer.get_config()\n",
    "        name = original_layer_config[\"name\"]\n",
    "\n",
    "        kwargs.pop(\"name\", None)\n",
    "\n",
    "        super().__init__(name=name, trainable=trainable, **kwargs)\n",
    "\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "\n",
    "        self._scale = alpha / rank\n",
    "\n",
    "        self._num_heads = original_layer_config[\"output_shape\"][-2]\n",
    "        self._hidden_dim = self._num_heads * original_layer_config[\"output_shape\"][-1]\n",
    "\n",
    "        # Layers.\n",
    "\n",
    "        # Original dense layer.\n",
    "        self.original_layer = original_layer\n",
    "        # No matter whether we are training the model or are in inference mode,\n",
    "        # this layer should be frozen.\n",
    "        self.original_layer.trainable = False\n",
    "\n",
    "        # LoRA dense layers.\n",
    "        self.A = keras.layers.Dense(\n",
    "            units=rank,\n",
    "            use_bias=False,\n",
    "            # Note: the original paper mentions that normal distribution was\n",
    "            # used for initialization. However, the official LoRA implementation\n",
    "            # uses \"Kaiming/He Initialization\".\n",
    "            kernel_initializer=keras.initializers.VarianceScaling(\n",
    "                scale=math.sqrt(5), mode=\"fan_in\", distribution=\"uniform\"\n",
    "            ),\n",
    "            trainable=trainable,\n",
    "            name=f\"lora_A\",\n",
    "        )\n",
    "        # B has the same `equation` and `output_shape` as the original layer.\n",
    "        # `equation = abc,cde->abde`, where `a`: batch size, `b`: sequence\n",
    "        # length, `c`: `hidden_dim`, `d`: `num_heads`,\n",
    "        # `e`: `hidden_dim//num_heads`. The only difference is that in layer `B`,\n",
    "        # `c` represents `rank`.\n",
    "        self.B = keras.layers.EinsumDense(\n",
    "            equation=original_layer_config[\"equation\"],\n",
    "            output_shape=original_layer_config[\"output_shape\"],\n",
    "            kernel_initializer=\"zeros\",\n",
    "            trainable=trainable,\n",
    "            name=f\"lora_B\",\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        original_output = self.original_layer(inputs)\n",
    "        if self.trainable:\n",
    "            # If we are fine-tuning the model, we will add LoRA layers' output\n",
    "            # to the original layer's output.\n",
    "            lora_output = self.B(self.A(inputs)) * self._scale\n",
    "            return original_output + lora_output\n",
    "\n",
    "        # If we are in inference mode, we \"merge\" the LoRA layers' weights into\n",
    "        # the original layer's weights - more on this in the text generation\n",
    "        # section!\n",
    "        return original_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6b963c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA-specific hyperparameters\n",
    "RANK = 4\n",
    "ALPHA = 32.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cae6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer_idx in range(lora_model.backbone.num_layers):\n",
    "    # Change query dense layer.\n",
    "    decoder_layer = lora_model.backbone.get_layer(f\"transformer_layer_{layer_idx}\")\n",
    "    self_attention_layer = decoder_layer._self_attention_layer\n",
    "\n",
    "    # Change query dense layer.\n",
    "    self_attention_layer._query_dense = LoraLayer(\n",
    "        self_attention_layer._query_dense,\n",
    "        rank=RANK,\n",
    "        alpha=ALPHA,\n",
    "        trainable=True,\n",
    "    )\n",
    "\n",
    "    # Change value dense layer.\n",
    "    self_attention_layer._value_dense = LoraLayer(\n",
    "        self_attention_layer._value_dense,\n",
    "        rank=RANK,\n",
    "        alpha=ALPHA,\n",
    "        trainable=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e2f5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in lora_model._flatten_layers():\n",
    "    lst_of_sublayers = list(layer._flatten_layers())\n",
    "\n",
    "    if len(lst_of_sublayers) == 1:  # \"leaves of the model\"\n",
    "        if layer.name in [\"lora_A\", \"lora_B\"]:\n",
    "            layer.trainable = True\n",
    "        else:\n",
    "            layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd1f82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47dbb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model.fit(imdb_train, validation_data=imdb_test, epochs=1,steps_per_epoch=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5a3037",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable = []\n",
    "for layer in lora_model._flatten_layers():\n",
    "    lst_of_sublayers = list(layer._flatten_layers())\n",
    "    if len(lst_of_sublayers) == 1:  # \"leaves of the model\"\n",
    "        if layer.name in [\"lora_A\", \"lora_B\"]:\n",
    "            for v in layer.trainable_variables:                \n",
    "                trainable.append(v.shape)\n",
    "\n",
    "params = 0\n",
    "for dim in trainable:\n",
    "    p = 1\n",
    "    for d in dim:\n",
    "        p *= d\n",
    "    params += p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff560ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1542116",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_sentiment(\n",
    "    [\"LoRA is very useful for quick LLM finetuning\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad398f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_sentiment(\n",
    "    [\"What an amazing movie!\", \"A total waste of my time.\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4ab3f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
